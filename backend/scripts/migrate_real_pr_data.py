#!/usr/bin/env python3
"""
è¿ç§»ç°æœ‰çœŸå®PRæ•°æ®åˆ°æ–°çš„ä¸‰è¡¨ç»“æ„
"""
import asyncio
import sys
import logging
from datetime import datetime
from sqlalchemy import text, select
from pathlib import Path

curr_dir = Path(__file__).resolve().parent
sys.path.append(str(curr_dir.parent))
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, '.')

from app.core.database import AsyncSessionLocal
from app.models.pr_metadata import PrMetadata, PrMetrics, PrStatus
from app.models.pr_lifecycle_event import PrLifecycleEvent, PrEventType
from app.models.user_identity import UserIdentity, IdentityPlatform, IdentityStatus

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class RealPrDataMigrator:
    """çœŸå®PRæ•°æ®è¿ç§»å™¨"""

    def __init__(self):
        self.stats = {
            'processed': 0,
            'pr_metadata_created': 0,
            'lifecycle_events_created': 0,
            'pr_metrics_created': 0,
            'identities_created': 0,
            'errors': []
        }

    async def migrate_real_data(self, limit: int = 10):
        """è¿ç§»çœŸå®æ•°æ®"""
        logger.info(f"ğŸš€ å¼€å§‹è¿ç§»ç°æœ‰çœŸå®PRæ•°æ® (é™åˆ¶: {limit}æ¡)")
        
        async with AsyncSessionLocal() as db:
            try:
                # 1. è·å–ç°æœ‰activitiesè¡¨ä¸­çš„PRæ•°æ®
                query = f"""
                    SELECT * FROM activities
                    WHERE id LIKE 'PR_%'
                    ORDER BY created_at DESC
                    LIMIT {limit}
                """
                result = await db.execute(text(query))
                activities = result.fetchall()
                
                logger.info(f"ğŸ“Š æ‰¾åˆ° {len(activities)} æ¡PRæ´»åŠ¨è®°å½•")
                
                if not activities:
                    logger.warning("æ²¡æœ‰æ‰¾åˆ°PRç›¸å…³çš„æ´»åŠ¨è®°å½•")
                    return
                
                # 2. è·å–å¯¹åº”çš„pull_requestsæ•°æ®
                pr_node_ids = [activity.id for activity in activities]
                pull_requests = {}
                if pr_node_ids:
                    pr_node_ids_str = "', '".join(pr_node_ids)
                    pr_query = f"""
                        SELECT * FROM pull_requests
                        WHERE pr_node_id IN ('{pr_node_ids_str}')
                    """
                    result = await db.execute(text(pr_query))
                    pull_requests = {pr.pr_node_id: pr for pr in result.fetchall()}

                    logger.info(f"ğŸ“Š æ‰¾åˆ° {len(pull_requests)} æ¡PRè®°å½•")
                
                # 3. è·å–PRç»“æœæ•°æ®
                pr_results = {}
                if pr_node_ids:
                    try:
                        results_query = f"""
                            SELECT * FROM pull_request_results
                            WHERE pr_node_id IN ('{pr_node_ids_str}')
                        """
                        result = await db.execute(text(results_query))
                        pr_results = {result.pr_node_id: result for result in result.fetchall()}

                        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(pr_results)} æ¡PRç»“æœè®°å½•")
                    except Exception as e:
                        logger.warning(f"è·å–PRç»“æœæ•°æ®å¤±è´¥: {e}")
                
                # 4. è¿ç§»æ¯ä¸ªPR
                for activity in activities:
                    await self._migrate_single_pr(
                        db, 
                        activity, 
                        pull_requests.get(activity.id),
                        pr_results.get(activity.id)
                    )
                    self.stats['processed'] += 1
                
                await db.commit()
                logger.info("âœ… çœŸå®æ•°æ®è¿ç§»å®Œæˆ")
                self._print_stats()
                
            except Exception as e:
                await db.rollback()
                logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}")
                self.stats['errors'].append(str(e))
                raise

    async def _migrate_single_pr(self, db, activity, pull_request, pr_result):
        """è¿ç§»å•ä¸ªPR"""
        try:
            pr_node_id = activity.id
            logger.info(f"ğŸ”„ è¿ç§»PR: {pr_node_id}")
            
            # 1. åˆ›å»ºç”¨æˆ·èº«ä»½ï¼ˆå¦‚æœéœ€è¦ï¼‰
            author_identity = None
            if pull_request:
                author_identity = await self._create_or_get_user_identity(db, activity.user_id)
            
                # 2. åˆ›å»ºPRå…ƒæ•°æ®
                pr_metadata = PrMetadata(
                    pr_node_id=pr_node_id,
                    pr_number=pull_request.pr_number,
                    repository=pull_request.repository,
                    title=activity.title,
                    description=activity.description or "",
                    author_identity_id=author_identity.id,
                    author_platform_username=author_identity.platform_username,
                    head_commit_sha=pull_request.commit_sha,
                    base_commit_sha="unknown",
                    commit_message=pull_request.commit_message,
                    files_changed=0,
                    additions=0,
                    deletions=0,
                    github_url=f"https://github.com/{pull_request.repository}/pull/{pull_request.pr_number}" if pull_request else "",
                    diff_url=activity.diff_url or "",
                    patch_url="",
                    github_created_at=self._parse_datetime(activity.created_at),
                    github_updated_at=self._parse_datetime(activity.updated_at),
                    created_at=datetime.utcnow().replace(microsecond=0)
                )
                db.add(pr_metadata)
                self.stats['pr_metadata_created'] += 1
            
            # 3. åˆ›å»ºç”Ÿå‘½å‘¨æœŸäº‹ä»¶
            event_time = self._parse_datetime(activity.created_at)
            lifecycle_event = PrLifecycleEvent(
                pr_node_id=pr_node_id,
                event_type=PrEventType.CREATED.value,
                event_time=event_time,
                event_source=IdentityPlatform.GITHUB.value,
                event_data={
                    "original_status": activity.status,
                    "activity_type": activity.activity_type
                },
                created_at=event_time
            )
            
            db.add(lifecycle_event)
            self.stats['lifecycle_events_created'] += 1
            
            # å¦‚æœæœ‰å®Œæˆæ—¶é—´ï¼Œæ·»åŠ å®Œæˆäº‹ä»¶
            if activity.completed_at:
                completed_event = PrLifecycleEvent(
                    pr_node_id=pr_node_id,
                    event_type=PrEventType.MERGED.value,
                    event_data={"source": "migration"},
                    created_at=self._parse_datetime(activity.completed_at)
                )
                db.add(completed_event)
                self.stats['lifecycle_events_created'] += 1
            
            # 4. åˆ›å»ºPRæŒ‡æ ‡
            # åŸºç¡€æŒ‡æ ‡
            total_score = 0.0
            code_quality_score = 0.0
            innovation_score = 0.0
            observability_score = 0.0
            performance_optimization_score = 0.0

            # å¦‚æœæœ‰PRç»“æœæ•°æ®ï¼Œè§£æAIåˆ†æç»“æœ
            if pr_result and hasattr(pr_result, 'ai_analysis_result') and pr_result.ai_analysis_result:
                try:
                    import json
                    data = json.loads(pr_result.ai_analysis_result)
                    total_score = data.get('overall_score', 0.0)
                    dimensions = data.get('dimensions', {})

                    code_quality = dimensions.get('code_quality', {})
                    innovation = dimensions.get('innovation', {})
                    observability = dimensions.get('observability', {})
                    performance_optimization = dimensions.get('performance_optimization', {})

                    code_quality_score = code_quality.get('score', 0.0) if code_quality else 0.0
                    innovation_score = innovation.get('score', 0.0) if innovation else 0.0
                    observability_score = observability.get('score', 0.0) if observability else 0.0
                    performance_optimization_score = performance_optimization.get('score', 0.0) if performance_optimization else 0.0
                except Exception as e:
                    logger.warning(f"è§£æAIåˆ†æç»“æœå¤±è´¥: {e}")

            pr_metrics = PrMetrics(
                pr_node_id=pr_node_id,
                current_status=PrStatus.MERGED.value if activity.status == 'completed' else PrStatus.OPEN.value,
                total_score=total_score,
                code_quality=code_quality_score,
                innovation=innovation_score,
                observability=observability_score,
                performance_optimization=performance_optimization_score,
                total_points=activity.points or 0,
                created_at=datetime.utcnow().replace(microsecond=0),
                updated_at=datetime.utcnow().replace(microsecond=0)
            )

            db.add(pr_metrics)
            self.stats['pr_metrics_created'] += 1
            
            logger.info(f"âœ… æˆåŠŸè¿ç§»PR: {pr_node_id}")
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»PR {activity.id} å¤±è´¥: {e}")
            self.stats['errors'].append(f"PR {activity.id}: {str(e)}")

    async def _create_or_get_user_identity(self, db, user_id):
        """åˆ›å»ºæˆ–è·å–ç”¨æˆ·èº«ä»½"""
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        result = await db.execute(
            select(UserIdentity).where(
                UserIdentity.platform == IdentityPlatform.GITHUB.value,
                UserIdentity.user_id == user_id,
            )
        )
        existing = result.scalar_one_or_none()

        if existing:
            return existing
        else:
            return False

    def _parse_datetime(self, dt_value):
        """è§£ææ—¥æœŸæ—¶é—´å€¼"""
        if dt_value is None:
            return datetime.utcnow().replace(microsecond=0)

        if isinstance(dt_value, str):
            try:
                # å°è¯•è§£æå­—ç¬¦ä¸²æ ¼å¼çš„æ—¥æœŸæ—¶é—´
                parsed_dt = datetime.fromisoformat(dt_value.replace('Z', '+00:00'))
                return parsed_dt.replace(microsecond=0)
            except ValueError:
                try:
                    # å°è¯•å…¶ä»–å¸¸è§æ ¼å¼
                    parsed_dt = datetime.strptime(dt_value, '%Y-%m-%d %H:%M:%S')
                    return parsed_dt.replace(microsecond=0)
                except ValueError:
                    logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ—¶é—´: {dt_value}")
                    return datetime.utcnow().replace(microsecond=0)

        if isinstance(dt_value, datetime):
            return dt_value.replace(microsecond=0)

        return datetime.utcnow().replace(microsecond=0)

    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("ğŸ“Š è¿ç§»ç»Ÿè®¡:")
        logger.info(f"  å¤„ç†çš„PRæ•°é‡: {self.stats['processed']}")
        logger.info(f"  åˆ›å»ºçš„PRå…ƒæ•°æ®: {self.stats['pr_metadata_created']}")
        logger.info(f"  åˆ›å»ºçš„ç”Ÿå‘½å‘¨æœŸäº‹ä»¶: {self.stats['lifecycle_events_created']}")
        logger.info(f"  åˆ›å»ºçš„PRæŒ‡æ ‡: {self.stats['pr_metrics_created']}")
        logger.info(f"  åˆ›å»ºçš„ç”¨æˆ·èº«ä»½: {self.stats['identities_created']}")
        logger.info(f"  é”™è¯¯æ•°é‡: {len(self.stats['errors'])}")

        if self.stats['errors']:
            logger.error("é”™è¯¯è¯¦æƒ…:")
            for error in self.stats['errors']:
                logger.error(f"  - {error}")


async def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    limit = 10  # é»˜è®¤è¿ç§»10æ¡
    if len(sys.argv) > 1:
        try:
            limit = int(sys.argv[1])
        except ValueError:
            logger.error("å‚æ•°å¿…é¡»æ˜¯æ•°å­—")
            sys.exit(1)
    
    migrator = RealPrDataMigrator()
    await migrator.migrate_real_data(limit)


if __name__ == "__main__":
    asyncio.run(main())
